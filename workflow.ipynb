{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_file_path = '../../dataset/Tests/video2_1_modified.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOT BOUNDARY DETECTION\n",
    "\n",
    "import cv2\n",
    "from scenedetect import VideoManager\n",
    "from scenedetect import SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip\n",
    "import json\n",
    "\n",
    "def get_video_duration(video_file_path):\n",
    "    with VideoFileClip(video_file_path) as video:\n",
    "        return video.duration  # duration in seconds\n",
    "\n",
    "def find_subarray_np(main_array, sub_array):\n",
    "    main_array = np.array(main_array)\n",
    "    sub_array = np.array(sub_array)\n",
    "    sub_len = len(sub_array)\n",
    "\n",
    "    # print(main_array)\n",
    "    # print(sub_array)\n",
    "\n",
    "    strided = np.lib.stride_tricks.sliding_window_view(main_array, window_shape=sub_len)\n",
    "\n",
    "    matches = np.all(strided == sub_array, axis=1)\n",
    "\n",
    "    indices = np.where(matches)[0]\n",
    "    \n",
    "    if indices.size > 0:\n",
    "        return indices\n",
    "    else:\n",
    "        return [-1]\n",
    "\n",
    "# Function to parse timecodes into timedelta objects\n",
    "def parse_timecode(time_str):\n",
    "    # Format: 'HH:MM:SS.sss'\n",
    "    return datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "\n",
    "# Function to convert seconds into hh:mm:ss.sss format\n",
    "def seconds_to_timestamp(seconds):\n",
    "    td = timedelta(seconds=seconds)\n",
    "    str_time = str(td)\n",
    "    hours, minutes, seconds = str_time.split(':')\n",
    "    seconds, microseconds = seconds.split('.')\n",
    "    milliseconds = f\"{int(microseconds):03d}\"[:3]\n",
    "    return f\"{hours}:{minutes}:{seconds}.{milliseconds}\"\n",
    "\n",
    "# Function to compute differences in timestamps\n",
    "def compute_differences(timestamps):\n",
    "    times = [parse_timecode(t) for t in timestamps]\n",
    "    \n",
    "    differences = []\n",
    "    for i in range(1, len(times)):\n",
    "        diff = (times[i] - times[i-1]).total_seconds()\n",
    "        differences.append(round(diff, 2))\n",
    "    return differences\n",
    "\n",
    "def find_scenes(video_path, threshold=30.0):\n",
    "    shot_boundaries = []\n",
    "\n",
    "    # Create a video manager object for the video.\n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    \n",
    "    # Add the ContentDetector algorithm (with a threshold setting).\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "    \n",
    "    # Start the video manager and perform scene detection.\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    # Detect scenes and return a list of scenes.\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    \n",
    "    # Obtain the scenes by frame and timecode.\n",
    "    scene_list = scene_manager.get_scene_list(video_manager.get_base_timecode())\n",
    "    \n",
    "    # Each scene is a tuple of (start, end) FrameTimecodes.\n",
    "    # print('List of scene changes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        shot_boundaries.append(scene[1].get_timecode())\n",
    "        #print(f'Scene {i+1}: Start {scene[0].get_timecode()} - End {scene[1].get_timecode()}')\n",
    "\n",
    "    video_manager.release()\n",
    "    return [shot_boundaries, scene_list]\n",
    "\n",
    "def compute_time_difference(time1, time2):\n",
    "    datetime1 = parse_timecode(time1)\n",
    "    datetime2 = parse_timecode(time2)\n",
    "\n",
    "    difference = datetime1 - datetime2 if datetime1 > datetime2 else datetime2 - datetime1\n",
    "\n",
    "    return difference.total_seconds()\n",
    "\n",
    "def get_fps(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    video.release()\n",
    "    return fps\n",
    "\n",
    "def timecode_to_frames(timecode, fps):\n",
    "    time_obj = parse_timecode(timecode)\n",
    "    total_seconds = time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1e6\n",
    "    frame_number = int(round(total_seconds * fps))\n",
    "    return frame_number\n",
    "\n",
    "\n",
    "query_video_boundaries, query_scenes = find_scenes(query_file_path)\n",
    "query_video_differences = compute_differences(query_video_boundaries)[:-1]\n",
    "\n",
    "if(query_video_differences == []):\n",
    "    print(\"No Shot Boundaries present in the query video\")\n",
    "else:\n",
    "    print(\"Query Video contains Shot Boundaries.\\nStarting shot boundary match now:\")\n",
    "    # Read the JSON file\n",
    "    with open('signatures/shotBoundSignature.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Get the arrays from the data\n",
    "    arrays = data['arrays']\n",
    "\n",
    "    # Iterate over each array\n",
    "    for key, array in arrays.items():\n",
    "        print(\"Array\", key + \":\")\n",
    "        original_video_boundaries = array\n",
    "        original_video_differences = compute_differences(original_video_boundaries)\n",
    "        # print(original_video_differences)\n",
    "\n",
    "        if(len(original_video_differences) < len(query_video_differences)):\n",
    "            print(\"No shot Boundary match found\\n\")\n",
    "            continue\n",
    "\n",
    "        start_index = find_subarray_np(original_video_differences, query_video_differences)[0]\n",
    "\n",
    "        # print(original_video_boundaries)\n",
    "        # print(query_video_boundaries)\n",
    "\n",
    "        if(start_index == -1):\n",
    "            print(\"No shot Boundary match found\\n\")\n",
    "        else:\n",
    "            print(\"Shot Boundary match found at index\", start_index)\n",
    "            print(key + \" is under consideration\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCEPTUAL IMAGE HASHING\n",
    "\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    \"\"\" Extracts a fixed number of frames evenly spaced from a video. \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if no more frames are available\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def calculate_hashes(frames):\n",
    "    \"\"\" Calculate perceptual hash for each frame. \"\"\"\n",
    "    return [imagehash.average_hash(Image.fromarray(frame)) for frame in frames]\n",
    "\n",
    "def compare_hashes(query_hashes, candidate_hashes):\n",
    "    \"\"\" Compare hashes from the query video to hashes from one candidate video using optimized numpy operations. \"\"\"\n",
    "    n = len(query_hashes)\n",
    "    min_diff = float('inf')\n",
    "\n",
    "    # Convert imagehash objects to NumPy arrays of the entire list\n",
    "    query_hashes_np = np.stack([np.array(h.hash, dtype=int) for h in query_hashes])\n",
    "    candidate_hashes_np = np.stack([np.array(h.hash, dtype=int) for h in candidate_hashes])\n",
    "\n",
    "    # Calculate the windowed sum of differences over all possible subarrays of length n\n",
    "    for offset in range(len(candidate_hashes) - n + 1):\n",
    "        # Select the window segment of candidate hashes\n",
    "        window = candidate_hashes_np[offset:offset + n]\n",
    "        # Calculate the number of different bits (hamming distance) using XOR and sum\n",
    "        current_diff = np.sum(query_hashes_np != window)\n",
    "        if current_diff < min_diff:\n",
    "            min_diff = current_diff\n",
    "\n",
    "    return min_diff\n",
    "\n",
    "\n",
    "def find_source_video(query_video_path):\n",
    "    query_frames = extract_frames(query_video_path)\n",
    "    query_hashes = calculate_hashes(query_frames)\n",
    "\n",
    "    # print(len(query_frames))\n",
    "    # print(len(query_hashes))\n",
    "\n",
    "    json_file_path = 'signatures/perceptualHash.json'  # Specify your file path here\n",
    "\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        precomputed_hashes = json.load(file)\n",
    "\n",
    "    best_match = None\n",
    "    smallest_diff = float('inf')\n",
    "\n",
    "    # Compare the query hashes against each candidate video's hashes in the JSON file\n",
    "    for video_file, candidate_hash_strings in precomputed_hashes.items():\n",
    "        print(\"Checking \" + video_file)\n",
    "        candidate_hashes = [imagehash.hex_to_hash(h_str) for h_str in candidate_hash_strings]\n",
    "\n",
    "        # Compare hashes to find the best match\n",
    "        diff = compare_hashes(query_hashes, candidate_hashes)\n",
    "        if diff < smallest_diff:\n",
    "            smallest_diff = diff\n",
    "            best_match = video_file\n",
    "\n",
    "    return best_match\n",
    "\n",
    "\n",
    "source_video = find_source_video(query_file_path)\n",
    "source_name = os.path.splitext(source_video)[0]\n",
    "print(f\"The query video is most likely from: {source_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUDIO - MFCC\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "\n",
    "def extract_audio(video_file_path, output_audio_path):\n",
    "    video = VideoFileClip(video_file_path)\n",
    "\n",
    "    audio = video.audio\n",
    "\n",
    "    audio.write_audiofile(output_audio_path, codec='pcm_s16le')\n",
    "\n",
    "    video.close()\n",
    "    audio.close()\n",
    "\n",
    "def find_best_match(input_features, query_features):\n",
    "    best_match = {'score': np.inf, 'index': 0}\n",
    "    num_frames = input_features.shape[1] - query_features.shape[1] + 1\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        current_window = input_features[:, i:i + query_features.shape[1]]\n",
    "        distance = np.linalg.norm(query_features - current_window)\n",
    "\n",
    "        if distance < best_match['score']:\n",
    "            best_match['score'] = distance\n",
    "            best_match['index'] = i\n",
    "\n",
    "    return best_match['index'], best_match['score']\n",
    "\n",
    "# Function to convert seconds into hh:mm:ss.sss format\n",
    "def seconds_to_timestamp(seconds):\n",
    "    if(seconds == 0.0):\n",
    "        return \"00:00:00.000\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    str_time = str(td)\n",
    "    hours, minutes, seconds = str_time.split(':')\n",
    "    seconds, microseconds = seconds.split('.')\n",
    "    milliseconds = f\"{int(microseconds):03d}\"[:3]\n",
    "    return f\"{hours}:{minutes}:{seconds}.{milliseconds}\"\n",
    "\n",
    "# functions to convert the timestamp into frame number in the original video\n",
    "def get_fps(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    video.release()\n",
    "    return fps\n",
    "\n",
    "def parse_timecode(time_str):\n",
    "    return datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "\n",
    "def timecode_to_frames(timecode, fps):\n",
    "    time_obj = parse_timecode(timecode)\n",
    "    total_seconds = time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second + time_obj.microsecond / 1e6\n",
    "    frame_number = int(round(total_seconds * fps))\n",
    "    return frame_number\n",
    "\n",
    "def load_array_from_json(filename):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "\n",
    "extract_audio(query_file_path, 'output_query_audio.wav')\n",
    "\n",
    "\n",
    "query_audio, query_sampling_rate = librosa.load('output_query_audio.wav')\n",
    "query_mfcc = librosa.feature.mfcc(y=query_audio, sr=query_sampling_rate, n_mfcc=13)\n",
    "\n",
    "# Load all arrays from JSON file\n",
    "loaded_data = np.load('signatures/mfcc_arrays.npz')\n",
    "# Iterate over each key-value pair in the JSON file\n",
    "\n",
    "key = source_name\n",
    "\n",
    "input_mfcc = loaded_data[key]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "start_index, similarity_score = find_best_match(input_mfcc, query_mfcc)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken: \", seconds_to_timestamp(end_time - start_time))\n",
    "\n",
    "HOP_LENGTH = 512\n",
    "fps = get_fps(query_file_path)\n",
    "\n",
    "start_time_seconds = start_index * HOP_LENGTH / query_sampling_rate\n",
    "query_duration_seconds = len(query_audio) / query_sampling_rate\n",
    "end_time_seconds = start_time_seconds + query_duration_seconds\n",
    "\n",
    "start_time = seconds_to_timestamp(start_time_seconds)\n",
    "end_time = seconds_to_timestamp(end_time_seconds)\n",
    "start_frame_audio = timecode_to_frames(start_time, fps)\n",
    "end_frame_audio = timecode_to_frames(end_time, fps)\n",
    "\n",
    "print(f\"For key: {key}\")\n",
    "print(f\"Start Time: {start_time} seconds\")\n",
    "print(f\"End Time: {end_time} seconds\")\n",
    "print(f\"Start Frame: {start_frame_audio}\")\n",
    "print(f\"End Frame: {end_frame_audio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOTION - OPTICAL FLOW\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "def resize_frame(frame, target_size=(640, 480)):\n",
    "    \"\"\" Resize the frame to a target size while maintaining aspect ratio. \"\"\"\n",
    "    dimensions = (target_size[1], target_size[0])  # OpenCV uses width first\n",
    "    return cv2.resize(frame, dimensions, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def extract_optical_flow_features(video_path, target_size):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read the first frame.\")\n",
    "        return []\n",
    "   \n",
    "    prev_frame = resize_frame(prev_frame, target_size)\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    features = []\n",
    "\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    pbar = tqdm(total=frame_count, desc=f'Extracting features- {video_path}')\n",
    "\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "       \n",
    "        frame = resize_frame(frame, target_size)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        flow_mag, flow_ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        mag_hist = np.histogram(flow_mag, bins=30, range=(0, 30))[0]\n",
    "        ang_hist = np.histogram(flow_ang, bins=30, range=(0, 2 * np.pi))[0]\n",
    "        features.append(np.concatenate((mag_hist, ang_hist)))\n",
    "\n",
    "        prev_gray = gray\n",
    "        pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    return np.array(features)\n",
    "\n",
    "def find_start_frame(input_features, query_features):\n",
    "    # Simple sliding window search\n",
    "    min_diff = float('inf')\n",
    "    start_frame = -1\n",
    "    for i in range(len(input_features) - len(query_features) + 1):\n",
    "        diff = np.sum((input_features[i:i+len(query_features)] - query_features)**2)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            start_frame = i\n",
    "\n",
    "    return start_frame\n",
    "\n",
    "def get_video_dimensions(video_path):\n",
    "    \"\"\" Get dimensions of the video. \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Could not read frame from video.\")\n",
    "        cap.release()\n",
    "        return None\n",
    "    height, width = frame.shape[:2]\n",
    "    cap.release()\n",
    "    return (width, height)\n",
    "\n",
    "\n",
    "query_dimensions = get_video_dimensions(query_file_path)\n",
    "print(query_dimensions)\n",
    "# Extract features\n",
    "query_features = extract_optical_flow_features(query_file_path, query_dimensions)\n",
    "\n",
    "filename = 'signatures/opticalFlow.json'\n",
    "with open(filename, 'r') as file:\n",
    "    existing_data = json.load(file)\n",
    "\n",
    "features = {}\n",
    "for key, value in existing_data.items():\n",
    "    features[key] = np.array(value)\n",
    "\n",
    "\n",
    "\n",
    "input_features = features[source_name]\n",
    "\n",
    "\n",
    "# Find the starting frame\n",
    "start_frame_motion = find_start_frame(input_features, query_features)\n",
    "print(f\"The query video starts at frame {start_frame_motion} of the input video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Player\n",
    "\n",
    "import sys\n",
    "import vlc\n",
    "from PyQt5 import QtWidgets, QtGui\n",
    "from PyQt5.QtGui import QIcon, QPixmap\n",
    "from PyQt5.QtCore import *\n",
    "\n",
    "class VideoPlayer(QtWidgets.QMainWindow):\n",
    "    def __init__(self, master, video_path):\n",
    "        super().__init__(master)\n",
    "        self.setWindowTitle(\"PyQt VLC Video Player\")\n",
    "    \n",
    "        self.instance = vlc.Instance()\n",
    "        self.player = self.instance.media_player_new()\n",
    "\n",
    "        self.central_widget = QtWidgets.QWidget(self)\n",
    "        self.setCentralWidget(self.central_widget)\n",
    "\n",
    "        self.layout = QtWidgets.QVBoxLayout()\n",
    "        self.layout_button = QtWidgets.QHBoxLayout()\n",
    "        self.central_widget.setLayout(self.layout)\n",
    "\n",
    "        self.frame = QtWidgets.QFrame()\n",
    "        self.layout.addWidget(self.frame)\n",
    "        if sys.platform == \"win32\":\n",
    "            self.player.set_hwnd(self.frame.winId())\n",
    "\n",
    "        btn_size = QSize(150, 50)\n",
    "    \n",
    "\n",
    "\n",
    "        self.play_button = QtWidgets.QPushButton()\n",
    "        self.play_button.clicked.connect(self.toggle_play_pause)\n",
    "        self.play_button.setFixedSize(btn_size)\n",
    "        pixmap = QPixmap(\"icons/play.png\")\n",
    "        self.play_button.setIcon(QIcon(pixmap))\n",
    "        self.play_button.setIconSize(btn_size)\n",
    "        self.layout_button.addWidget(self.play_button)\n",
    "        \n",
    "        self.stop_button = QtWidgets.QPushButton()\n",
    "        self.stop_button.clicked.connect(self.stop_player)\n",
    "        self.stop_button.setFixedSize(btn_size)\n",
    "        pixmap = QPixmap(\"icons/stop.png\")\n",
    "        self.stop_button.setIcon(QIcon(pixmap))\n",
    "        self.stop_button.setIconSize(btn_size)\n",
    "        self.layout_button.addWidget(self.stop_button)\n",
    "        \n",
    "        self.skip_button = QtWidgets.QPushButton()\n",
    "        self.skip_button.clicked.connect(self.skip_to)\n",
    "        self.skip_button.setFixedSize(btn_size)\n",
    "        pixmap = QPixmap(\"icons/fastfwd.png\")\n",
    "        self.skip_button.setIcon(QIcon(pixmap))\n",
    "        self.skip_button.setIconSize(btn_size)\n",
    "        self.layout_button.addWidget(self.skip_button)\n",
    "\n",
    "\n",
    "        self.layout.addLayout(self.layout_button)\n",
    "\n",
    "\n",
    "        self.media = self.instance.media_new(video_path)\n",
    "        self.player.set_media(self.media)\n",
    "    \n",
    "        self.setGeometry(100, 100, 1400, 900)\n",
    "        self.show()\n",
    "\n",
    "    def toggle_play_pause(self):\n",
    "        btn_size = QSize(150, 50)\n",
    "        if self.player.is_playing():\n",
    "            self.player.pause()\n",
    "            pixmap = QPixmap(\"icons/play.png\")\n",
    "            self.play_button.setIcon(QIcon(pixmap))\n",
    "            self.play_button.setIconSize(btn_size)\n",
    "        else:\n",
    "            self.player.play()\n",
    "            pixmap = QPixmap(\"icons/pause.png\")\n",
    "            self.play_button.setIcon(QIcon(pixmap))\n",
    "            self.play_button.setIconSize(btn_size)\n",
    "    \n",
    "    def stop_player(self):\n",
    "        self.player.stop()\n",
    "    \n",
    "    def skip_to(self):\n",
    "        fps = self.player.get_fps()\n",
    "\n",
    "        frame_number, okPressed = QtWidgets.QInputDialog.getInt(self, \"Skip to frame\",\"Frame number:\", 0, 0, 100000, 1)\n",
    "        \n",
    "        if okPressed:\n",
    "            ms_time = (frame_number / fps) * 1000\n",
    "            self.player.set_time(int(ms_time))\n",
    "\n",
    "def main():\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    video_path = 'dataset/originals/video1.mp4' \n",
    "    player = VideoPlayer(None, video_path)\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
